---
title: 'IDS702 Final Project: HomeCredit Default Risk Dataset'
author: "William Huang"
date: "November 28, 2018"
output: html_document
---

##Project Objective
Home Credit Group, founded in Czech Republic a d headquartered in Netherlands, is a multinational non-bank financial company that provides consumer financial products, such as personal lending and credit card businesses. The company focuses primarily on people with no or little credit history. The project objective is to interpret how the information on the loan applications could affect the default risks of loan applicants.

####Variable Descriptions
- **TARGET**: Target variable (1 - client with payment difficulties: he/she had late payment more than X days on at least one of the first Y installments of the loan in our sample, 0 - all other cases)
- **CODE_GENDER**: Gender of the client
- **NAME_CONTRACT_TYPE**: Identification if loan is cash or revolving
- **AMT_INCOME_TOTAL**: Income of the client
- **FLAG_OWN_CAR**: Flag if the client owns a car
- **FLAG_OWN_REALTY**: Flag if client owns a house or flat
- **CNT_CHILDREN**: Flag if client owns a house or flat
- **AMT_CREDIT**: Credit amount of the loan
- **AMT_ANNUITY**: Loan annuity
- **AMT_GOODS_PRICE**: For consumer loans it is the price of the goods for which the loan is given
- **NAME_TYPE_SUITE**: Who was accompanying client when he was applying for the loan
- **NAME_INCOME_TYPE**: Clients income type (businessman, working, maternity leave,â€¦)
- **NAME_EDUCATION_TYPE**: Level of highest education the client achieved
- **NAME_FAMILY_STATUS**: Family status of the client
- **NAME_HOUSING_TYPE**: What is the housing situation of the client (renting, living with parents, ...)
- **REGION_POPULATION_RELATIVE**: Normalized population of region where client lives (higher number means the client lives in more populated region)
- **DAYS_BIRTH**: Client's age in days at the time of application
- **DAYS_EMPLOYED**: How many days before the application the person started current employment
- **OWN_CAR_AGE**: Age of client's car
- **OCCUPATION_TYPE**: What kind of occupation does the client have
- **CNT_FAM_MEMBERS**: How many family members does client have
- **ORGANIZATION_TYPE**: Type of organization where client works
- **AMT_REQ_CREDIT_BUREAU_HOUR**: Number of enquiries to Credit Bureau about the client one hour before application
- **AMT_REQ_CREDIT_BUREAU_DAY**: Number of enquiries to Credit Bureau about the client one day before application (excluding one hour before application)
- **AMT_REQ_CREDIT_BUREAU_WEEK**: Number of enquiries to Credit Bureau about the client one week before application (excluding one day before application)
- **AMT_REQ_CREDIT_BUREAU_MON**: Number of enquiries to Credit Bureau about the client one month before application (excluding one week before application)
- **AMT_REQ_CREDIT_BUREAU_QRT**: Number of enquiries to Credit Bureau about the client 3 month before application (excluding one month before application)
- **AMT_REQ_CREDIT_BUREAU_YEAR**: Number of enquiries to Credit Bureau about the client one day year (excluding last 3 months before application)

###Load Data and Data Pre-processing
As the original dataset contains over 200 independent variables, I picked 23 variables for further analysis and interpretations based on my interests and previous experience in this industry.
Besides, I also combined the following 6 variables together to improve the interpretability of my model: `AMT_REQ_CREDIT_BUREAU_HOUR`, `AMT_REQ_CREDIT_BUREAU_DAY`, `AMT_REQ_CREDIT_BUREAU_WEEK`, `AMT_REQ_CREDIT_BUREAU_MON`, `AMT_REQ_CREDIT_BUREAU_QRT`, `AMT_REQ_CREDIT_BUREAU_YEAR`. 
These variables represent the number of enquiries to Credit Bureau about the client one hour, one day(excluding one hour before), one week(excluding one day before), one month(excluding one week before), one quarter(excluding one month before) and one year(excluding one quarter before) before application. As my goal is to construct an interpretation model, instead of a predictive model, the integration of these variables could enhance the interpretability of the model substantially. On the other hand, if I intended to build a predictive model, I should have kept them as 6 different variables.
```{r setup, include=TRUE}
library(mice)
library(dplyr)
library(tidyverse)
library(pROC)
library(arm)
#setwd("/Users/wh132/Desktop")
setwd("/Users/Macintosh/Desktop")
application_data <- read.csv("application_train.csv")
```
```{r}
#select interested variables
application_data_selected <- subset(application_data, select = c("TARGET","CODE_GENDER","NAME_CONTRACT_TYPE","AMT_INCOME_TOTAL","FLAG_OWN_CAR","FLAG_OWN_REALTY","CNT_CHILDREN","AMT_CREDIT", "AMT_ANNUITY", "AMT_GOODS_PRICE", "NAME_TYPE_SUITE","NAME_INCOME_TYPE","NAME_EDUCATION_TYPE","NAME_FAMILY_STATUS","NAME_HOUSING_TYPE","REGION_POPULATION_RELATIVE","DAYS_BIRTH","DAYS_EMPLOYED","OWN_CAR_AGE","OCCUPATION_TYPE","CNT_FAM_MEMBERS","ORGANIZATION_TYPE","AMT_REQ_CREDIT_BUREAU_HOUR","AMT_REQ_CREDIT_BUREAU_DAY","AMT_REQ_CREDIT_BUREAU_WEEK","AMT_REQ_CREDIT_BUREAU_MON","AMT_REQ_CREDIT_BUREAU_QRT","AMT_REQ_CREDIT_BUREAU_YEAR"))

#combine all the AMT_REQ_CREDIT_BUREAU variables together to compute the aggregated number of enquiries to Credit Bureau about the client one year before the application
application_data_selected$AMR_REQ_CREDIT_BUREAU_SUM <- application_data_selected$AMT_REQ_CREDIT_BUREAU_HOUR+application_data_selected$AMT_REQ_CREDIT_BUREAU_DAY+application_data_selected$AMT_REQ_CREDIT_BUREAU_MON+application_data_selected$AMT_REQ_CREDIT_BUREAU_QRT+application_data_selected$AMT_REQ_CREDIT_BUREAU_WEEK+application_data_selected$AMT_REQ_CREDIT_BUREAU_YEAR

#Drop AMT_REQ_CREDIT_BUREAU_HOUR, DAY, WEEK, MON, QRT, YEAR
application_data_selected <- application_data_selected %>%
    dplyr::select(-(AMT_REQ_CREDIT_BUREAU_HOUR)) %>%
    dplyr::select(-(AMT_REQ_CREDIT_BUREAU_DAY)) %>%
    dplyr::select(-(AMT_REQ_CREDIT_BUREAU_WEEK)) %>%
    dplyr::select(-(AMT_REQ_CREDIT_BUREAU_MON)) %>%
    dplyr::select(-(AMT_REQ_CREDIT_BUREAU_QRT)) %>%
    dplyr::select(-(AMT_REQ_CREDIT_BUREAU_YEAR))
```


Based on the correlation table above, two pairs of independent variables have extremely high correlations:

+ (1) (`AMT_CREDIT`, `AMT_GOODS_PRICE`), corr = 0.987 
`AMT_GOODS_PRICE` represents the goods price of good that client asked for on the previous application.
`AMT_CREDIT` represents the final credit amount on the previous application.
Through the definitions of these two variables, it's clear that we could drop one of the variables.

+ (2) (`CNT_FAM_MEMBERS`, `CNT_CHILDREN`), corr = 0.914
`CNT_FAM_mEMBERS` represents how many family members clients have
`CNT_CHILDREN` represents how many children clients have
Through the definitions of these two variables, it's clear that we should drop one of the variables.

```{r}
#check collinearity
num <- unlist(lapply(application_data_selected, is.numeric))
cor(cc(application_data_selected[,num]), use = "pair")
```

```{r}
application_data_selected <- application_data_selected %>%
    dplyr::select(-(AMT_GOODS_PRICE)) %>%
    dplyr::select(-(CNT_CHILDREN))
```


Here we should factorize `CNT_FAM_MEMBERS`, representing how many family members the loan applicant has, and `AMT_REQ_CREDIT_BUREAU_SUM`, representing number of enquiries to Credit Bureau about the client one year before submitting his loan application, as both variables are more like categorical rather than continuous based on their definitions. 
```{r}
#factorize CNT_FAM_MEMBERS, AMR_REQ_CREDIT_BUREAU_SUM
application_data_selected$CNT_FAM_MEMBERS <- as.factor(application_data_selected$CNT_FAM_MEMBERS)
application_data_selected$AMR_REQ_CREDIT_BUREAU_SUM <- as.factor(application_data_selected$AMR_REQ_CREDIT_BUREAU_SUM)
```

As the dataset is extremely large and my computer's computing power doesn't support this size of computations. Therefore, I decided to randomly select 10,000 observations to run further analysis. Due to the random selection process, the random selection process should be able to represent the population. The summary tables for original dataset and the dataset after my random selection process in the Data Description section below also support this claim.
```{r}
set.seed(100)
application_data_sam <- application_data_selected[sample(nrow(application_data_selected), 10000), ]
summary(application_data_sam)
```
##Data Description
Through the two tables below, we can see that the distributions of each variable in the original and new datasets are fairly similar. Besides, we also have some missing values and should consider missing value imputations.
```{r}
summary(application_data_selected) #original datasets with selected variables
```
```{r}
summary(application_data_sam) #the dataset after the random selection process
```

##Missing values imputation
`OWN_CAR_AGE` and `AMR_REQ_CREDOT_BUREAU_SUM` are the two columns that contain missing values in the dataset. I used the mice package to conduct missing value imputation to generate complete datasets. For the imputation method, I chose cart, instead of the default method. I have tried to use the default method to impute missing values; however, it returned the following error `"system is computationally singular"`. The cause of the problem here could probably be the large number of unbalanced factor variables, such as `NAME_CONTRACT_TYPE`, `NAME_TYPE_SUITE` and `NAME_EDUCATION_TYPE`, in the dataset. When these variables are turned into dummy variables, there's a high probability that one column is a linear combination of another. As the default imputation methods are parametric, which involve linear regression, this would result in a X matrix that cannot be inverted. Therefore, I considered to change the imputation method to Classification and Regression Trees (CART) that is not stochastic and non-parametric, which require no X matrix inversion. (Reference: [links](https://bit.ly/2QdvVzR))
```{r echo = T}
#check pattern
md.pattern(application_data_sam)
```
```{r, results = 'hide', warning = FALSE}
application_MI <- mice(application_data_sam, m = 10, method = "cart", seed = 8)
```
##Imputation Model Diagnostics
Based on the charts below, no problematic pattern with the imputations.
```{r}
stripplot(application_MI, col=c("grey",mdc(2)),pch=c(1,20))
stripplot(application_MI, OWN_CAR_AGE~TARGET, col=c("grey",mdc(2)),pch=c(1,20), xlab = 'TARGET', ylab = "OWN_CAR_AGE")
stripplot(application_MI, AMR_REQ_CREDIT_BUREAU_SUM~TARGET, col=c("grey",mdc(2)),pch=c(1,20), xlab = 'TARGET', ylab = "AMT_REQ_CREDIT_BUREAU_SUM")

```

##Posterior Predictive Check on Two Complete Datasets
Both the histogram and boxplots look similiar for replica and complete datasets; therefore, there is no evidence suggests that imputation models are poorly specified for what I want to do.
```{r, results = 'hide', warning = FALSE}
application_ppcheck <- rbind(application_data_sam, application_data_sam)
application_ppcheck[10001:20000, apply(is.na(application_data_sam), any, MARGIN = 2)] <- NA
application_ppcheck_MI <- mice(application_ppcheck, m = 10, method = "cart", seed = 12)
d1ppcheck <- mice::complete(application_ppcheck_MI, 1)
d2ppcheck <- mice::complete(application_ppcheck_MI, 2)
```
```{r, fig.width=5}
#dataset1
par(mfrow = c(1,2))
boxplot(d1ppcheck$OWN_CAR_AGE[1:10000]~d1ppcheck$TARGET[1:10000], ylab="OWN_CAR_AGE", xlab="TARGET", main = "OWN_CAR_AGE vs TARGET completed data")
boxplot(d1ppcheck$OWN_CAR_AGE[10001:20000]~d1ppcheck$TARGET[10001:20000], ylab="OWN_CAR_AGE", xlab="TARGET", main = "OWN_CAR_AGE vs TARGET completed data")
```
```{r, fig.width=5}
par(mfrow = c(2,1))
hist(as.numeric(d1ppcheck$AMR_REQ_CREDIT_BUREAU_SUM[1:10000]), xlab="AMR_REQ_CREDIT_BUREAU_SUM", main = "AMR_REQ_CREDIT_BUREAU_SUM complete data")
hist(as.numeric(d1ppcheck$AMR_REQ_CREDIT_BUREAU_SUM[10001:20000]), xlab="AMR_REQ_CREDIT_BUREAU_SUM", main = "AMR_REQ_CREDIT_BUREAU_SUM replicated data")
```

##Base Regression Model
Here I constructed a base logsitic regression model without any transformation and interaction term. I also selected three complete datasets to see AUCs for the fitted models. The AUCs are fairly good at 0.6981, 0.6993 and 0.6986. 
```{r, warning = FALSE}
reg <- with(data = application_MI, glm(TARGET ~ CODE_GENDER + NAME_CONTRACT_TYPE + AMT_INCOME_TOTAL + FLAG_OWN_CAR + FLAG_OWN_REALTY + AMT_CREDIT + AMT_ANNUITY + NAME_TYPE_SUITE + NAME_INCOME_TYPE + NAME_EDUCATION_TYPE + NAME_FAMILY_STATUS + NAME_HOUSING_TYPE + REGION_POPULATION_RELATIVE + DAYS_BIRTH + DAYS_EMPLOYED + OWN_CAR_AGE + OCCUPATION_TYPE + CNT_FAM_MEMBERS + ORGANIZATION_TYPE + AMR_REQ_CREDIT_BUREAU_SUM), family = binomial)
```

######Use Three Completed Datasets to See AUCs for the Fitted Models
```{r, warning = FALSE}
#Dataset 1
par(mfrow=c(1,1))
cd1 <- mice::complete(application_MI, 1)
reg_cd1 <- glm(data=cd1, TARGET~CODE_GENDER + NAME_CONTRACT_TYPE + AMT_INCOME_TOTAL + FLAG_OWN_CAR + FLAG_OWN_REALTY + AMT_CREDIT + AMT_ANNUITY + NAME_TYPE_SUITE + NAME_INCOME_TYPE + NAME_EDUCATION_TYPE + NAME_FAMILY_STATUS + NAME_HOUSING_TYPE + REGION_POPULATION_RELATIVE + DAYS_BIRTH + DAYS_EMPLOYED + OWN_CAR_AGE + OCCUPATION_TYPE + CNT_FAM_MEMBERS + ORGANIZATION_TYPE + AMR_REQ_CREDIT_BUREAU_SUM, family=binomial)
roc(cd1$TARGET, fitted(reg_cd1), plot=T, legacy.axes=T)
```
```{r, warning = FALSE}
#Dataset 2
cd2 <- mice::complete(application_MI, 2)
reg_cd2 <- glm(data=cd2, TARGET~CODE_GENDER + NAME_CONTRACT_TYPE + AMT_INCOME_TOTAL + FLAG_OWN_CAR + FLAG_OWN_REALTY + AMT_CREDIT + AMT_ANNUITY + NAME_TYPE_SUITE + NAME_INCOME_TYPE + NAME_EDUCATION_TYPE + NAME_FAMILY_STATUS + NAME_HOUSING_TYPE + REGION_POPULATION_RELATIVE + DAYS_BIRTH + DAYS_EMPLOYED + OWN_CAR_AGE + OCCUPATION_TYPE + CNT_FAM_MEMBERS + ORGANIZATION_TYPE + AMR_REQ_CREDIT_BUREAU_SUM,  family=binomial)
roc(cd2$TARGET, fitted(reg_cd2), plot=T, legacy.axes=T)
```
```{r, warning = FALSE}
#Dataset 3
cd3 <- mice::complete(application_MI, 3)
reg_cd3 <- glm(data=cd3, TARGET~CODE_GENDER + NAME_CONTRACT_TYPE + AMT_INCOME_TOTAL + FLAG_OWN_CAR + FLAG_OWN_REALTY + AMT_CREDIT + AMT_ANNUITY
               + NAME_TYPE_SUITE + NAME_INCOME_TYPE + NAME_EDUCATION_TYPE + NAME_FAMILY_STATUS + NAME_HOUSING_TYPE 
               + REGION_POPULATION_RELATIVE + DAYS_BIRTH + DAYS_EMPLOYED + OWN_CAR_AGE + OCCUPATION_TYPE + CNT_FAM_MEMBERS 
               + ORGANIZATION_TYPE + AMR_REQ_CREDIT_BUREAU_SUM, 
               family=binomial)
roc(cd3$TARGET, fitted(reg_cd3), plot=T, legacy.axes=T)
```

##Model Diagnostics
I used binned plots to check residuals to check validity of model assumptions for continuous variables. Based on plots below, there's no major violation of model assumptions, except for *Binned residuals versus DAYS_EMPLOYED*. The plot is largely skewed by some points. Therefore, I plotted another residual plot to investigate if the other points form any pattern that might violate model assumptions.   
```{r, fig.width=5}
par(mfrow = c(3,3))
#Dataset 1
cd1 <- mice::complete(application_MI, 1)
#Binned residual plots
rawresid1 = cd1$TARGET - fitted(reg_cd1)
#continuous variables
binnedplot(x=cd1$AMT_INCOME_TOTAL, y = rawresid1, xlab = "AMT_INCOME_TOTAL", ylab = "Residuals", 
           main = "Binned residuals versus AMT_INCOME_TOTAL")
binnedplot(x=cd1$AMT_CREDIT, y = rawresid1, xlab = "AMT_CREDIT", ylab = "Residuals", 
           main = "Binned residuals versus AMT_CREDIT")
binnedplot(x=cd1$AMT_ANNUITY, y = rawresid1, xlab = "AMT_ANNUITY", ylab = "Residuals", 
           main = "Binned residuals versus AMT_ANNUITY")
binnedplot(x=cd1$REGION_POPULATION_RELATIVE , y = rawresid1, xlab = "REGION_POPULATION_RELATIVE", ylab = "Residuals", 
           main = "Binned residuals versus REGION_POPULATION_RELATIVE")
binnedplot(x=cd1$DAYS_BIRTH , y = rawresid1, xlab = "DAYS_BIRTH", ylab = "Residuals", 
           main = "Binned residuals versus DAYS_BIRTH")
binnedplot(x=cd1$DAYS_EMPLOYED , y = rawresid1, xlab = "DAYS_EMPLOYED", ylab = "Residuals", 
           main = "Binned residuals versus DAYS_EMPLOYED")
binnedplot(x=cd1$OWN_CAR_AGE , y = rawresid1, xlab = "OWN_CAR_AGE", ylab = "Residuals", 
           main = "Binned residuals versus OWN_CAR_AGE")
binnedplot(x=as.numeric(cd1$AMR_REQ_CREDIT_BUREAU_SUM) , y = rawresid1, xlab = "OAMR_REQ_CREDIT_BUREAU_SUM", ylab = "Residuals", main = "Binned residuals versus AMR_REQ_CREDIT_BUREAU_SUM")

#DAYS_EMPLOYED
temp <- cd1[cd1$DAYS_EMPLOYED != 365243,]
binnedplot(x=temp$DAYS_EMPLOYED , y = rawresid1, xlab = "DAYS_EMPLOYED", ylab = "Residuals", 
           main = "Binned residuals versus DAYS_EMPLOYED")
```

After I excluded the large value in the `DAYS_EMPLOYED` columns(`application_data_sam$DAYS_EMPLOYED == 365,243`), the residual plot suggests no major violation of model assumptions. For those large value outliers, as they are also valid input data, I have no scientific reason to remove them.
```{r, fig.width=7}
par(mfrow = c(1,2))
binnedplot(x=cd1$DAYS_EMPLOYED[cd1$DAYS_EMPLOYED != max(application_data_sam$DAYS_EMPLOYED)] , y = rawresid1[cd1$DAYS_EMPLOYED != max(application_data_sam$DAYS_EMPLOYED)], xlab = "DAYS_EMPLOYED", ylab = "Residuals", 
           main = "Binned residuals versus DAYS_EMPLOYED (without outliers)")
binnedplot(x=cd1$DAYS_EMPLOYED , y = rawresid1, xlab = "DAYS_EMPLOYED", ylab = "Residuals", 
           main = "Binned residuals versus DAYS_EMPLOYED")
```

##Interaction Effect
There are several pairs of variables we would like to check for possible interaction effects based on the scientific reasons underneath.

- `NAME_INCOME_TYPE` versus `AMT_CREDIT`
The amount of credit a client has is typically based on his income type in the financial practice. For example, state servants' incomes are considered more stable and banks typically give them higher amounts of credits. Therefore, I believe there could be an interaction effect between these two variables.
However, after taking an F-test, we could conclude that the interaction effect is not significant and I should not incorporate it into my model.
```{r}
bwplot(as.factor(TARGET)~AMT_CREDIT|as.factor(NAME_INCOME_TYPE), data = cd1, ylab = "TARGET")
```

```{r}
reg_cd1 <- glm(data=cd1, TARGET~CODE_GENDER + NAME_CONTRACT_TYPE + AMT_INCOME_TOTAL + FLAG_OWN_CAR + FLAG_OWN_REALTY + AMT_CREDIT + AMT_ANNUITY + NAME_TYPE_SUITE + NAME_INCOME_TYPE + NAME_EDUCATION_TYPE + NAME_FAMILY_STATUS + NAME_HOUSING_TYPE + REGION_POPULATION_RELATIVE + DAYS_BIRTH + DAYS_EMPLOYED + OWN_CAR_AGE + OCCUPATION_TYPE + CNT_FAM_MEMBERS + ORGANIZATION_TYPE + AMR_REQ_CREDIT_BUREAU_SUM, family=binomial)
reg_int <- glm(data=cd1, TARGET~CODE_GENDER + NAME_CONTRACT_TYPE + AMT_INCOME_TOTAL + FLAG_OWN_CAR + FLAG_OWN_REALTY + AMT_CREDIT + AMT_ANNUITY + NAME_TYPE_SUITE + NAME_INCOME_TYPE + NAME_EDUCATION_TYPE + NAME_FAMILY_STATUS + NAME_HOUSING_TYPE + REGION_POPULATION_RELATIVE + DAYS_BIRTH + DAYS_EMPLOYED + OWN_CAR_AGE + OCCUPATION_TYPE + CNT_FAM_MEMBERS + ORGANIZATION_TYPE + AMR_REQ_CREDIT_BUREAU_SUM + AMT_CREDIT*NAME_INCOME_TYPE, family=binomial)
anova(reg_cd1, reg_int, test = "Chisq")
```

- `NAME_EDUCATION_TYPE` versus `AMT_INCOME_TOTAL`
The amount of income is typically associated with the highest education he or her received. Therefore, I try to see if there's any interaction effect between these two variables. The plots below suggest that there might be an interaction effect, though not very clear. But the F-test indicates that I should not add an interaction term between these two variables in the regression model.
```{r}
bwplot(as.factor(TARGET)~AMT_INCOME_TOTAL|as.factor(NAME_EDUCATION_TYPE), data = cd1, ylab = "TARGET")
```
```{r}
reg_cd1 <- glm(data=cd1, TARGET~CODE_GENDER + NAME_CONTRACT_TYPE + AMT_INCOME_TOTAL + FLAG_OWN_CAR + FLAG_OWN_REALTY + AMT_CREDIT + AMT_ANNUITY + NAME_TYPE_SUITE + NAME_INCOME_TYPE + NAME_EDUCATION_TYPE + NAME_FAMILY_STATUS + NAME_HOUSING_TYPE + REGION_POPULATION_RELATIVE + DAYS_BIRTH + DAYS_EMPLOYED + OWN_CAR_AGE + OCCUPATION_TYPE + CNT_FAM_MEMBERS + ORGANIZATION_TYPE + AMR_REQ_CREDIT_BUREAU_SUM, family=binomial)
reg_int <- glm(data=cd1, TARGET~CODE_GENDER + NAME_CONTRACT_TYPE + AMT_INCOME_TOTAL + FLAG_OWN_CAR + FLAG_OWN_REALTY + AMT_CREDIT + AMT_ANNUITY + NAME_TYPE_SUITE + NAME_INCOME_TYPE + NAME_EDUCATION_TYPE + NAME_FAMILY_STATUS + NAME_HOUSING_TYPE + REGION_POPULATION_RELATIVE + DAYS_BIRTH + DAYS_EMPLOYED + OWN_CAR_AGE + OCCUPATION_TYPE + CNT_FAM_MEMBERS + ORGANIZATION_TYPE + AMR_REQ_CREDIT_BUREAU_SUM + AMT_INCOME_TOTAL*NAME_EDUCATION_TYPE, family=binomial)
anova(reg_cd1, reg_int, test = "Chisq")
```

- `OCCUPATION_TYPE` versus `AMT_INCOME_TOTAL`
Different kinds of occupation generally have different levels of incomes. For example, high skill tech staffs typically have higher incomes than cleaning staffs. Hence, I think there might be an interaction effect. However, the F-test suggests that the interaction effect is not significant enough and I should not add it into the model.
```{r}
bwplot(as.factor(TARGET)~AMT_INCOME_TOTAL|as.factor(OCCUPATION_TYPE), data = cd1, ylab = "TARGET")
```

```{r}
reg_cd1 <- glm(data=cd1, TARGET~CODE_GENDER + NAME_CONTRACT_TYPE + AMT_INCOME_TOTAL + FLAG_OWN_CAR + FLAG_OWN_REALTY + AMT_CREDIT + AMT_ANNUITY + NAME_TYPE_SUITE + NAME_INCOME_TYPE + NAME_EDUCATION_TYPE + NAME_FAMILY_STATUS + NAME_HOUSING_TYPE + REGION_POPULATION_RELATIVE + DAYS_BIRTH + DAYS_EMPLOYED + OWN_CAR_AGE + OCCUPATION_TYPE + CNT_FAM_MEMBERS + ORGANIZATION_TYPE + AMR_REQ_CREDIT_BUREAU_SUM, family=binomial)
reg_int <- glm(data=cd1, TARGET~CODE_GENDER + NAME_CONTRACT_TYPE + AMT_INCOME_TOTAL + FLAG_OWN_CAR + FLAG_OWN_REALTY + AMT_CREDIT + AMT_ANNUITY + NAME_TYPE_SUITE + NAME_INCOME_TYPE + NAME_EDUCATION_TYPE + NAME_FAMILY_STATUS + NAME_HOUSING_TYPE + REGION_POPULATION_RELATIVE + DAYS_BIRTH + DAYS_EMPLOYED + OWN_CAR_AGE + OCCUPATION_TYPE + CNT_FAM_MEMBERS + ORGANIZATION_TYPE + AMR_REQ_CREDIT_BUREAU_SUM + AMT_INCOME_TOTAL*OCCUPATION_TYPE, family=binomial)
anova(reg_cd1, reg_int, test = "Chisq")
```

- `ORGANIZATION_TYPE` versus `AMT_CREDIT`
For most banks' practices, banks generally offer different credit amounts for people work in different organizations. For example, most banks offer Fortune 500 companies' employees more credits. As a result, we think there might be an interaction effect between these two variables. The plots below also suggests that there could be a potential interaction term to be added. However, the F-test indicates that the interaction term is not significant enough to be added into the regression model. 
```{r}
bwplot(as.factor(TARGET)~AMT_CREDIT|as.factor(ORGANIZATION_TYPE), data = cd1, ylab = "TARGET")
```

```{r}
reg_cd1 <- glm(data=cd1, TARGET~CODE_GENDER + NAME_CONTRACT_TYPE + AMT_INCOME_TOTAL + FLAG_OWN_CAR + FLAG_OWN_REALTY + AMT_CREDIT + AMT_ANNUITY + NAME_TYPE_SUITE + NAME_INCOME_TYPE + NAME_EDUCATION_TYPE + NAME_FAMILY_STATUS + NAME_HOUSING_TYPE + REGION_POPULATION_RELATIVE + DAYS_BIRTH + DAYS_EMPLOYED + OWN_CAR_AGE + OCCUPATION_TYPE + CNT_FAM_MEMBERS + ORGANIZATION_TYPE + AMR_REQ_CREDIT_BUREAU_SUM, family=binomial)
reg_int <- glm(data=cd1, TARGET~CODE_GENDER + NAME_CONTRACT_TYPE + AMT_INCOME_TOTAL + FLAG_OWN_CAR + FLAG_OWN_REALTY + AMT_CREDIT + AMT_ANNUITY + NAME_TYPE_SUITE + NAME_INCOME_TYPE + NAME_EDUCATION_TYPE + NAME_FAMILY_STATUS + NAME_HOUSING_TYPE + REGION_POPULATION_RELATIVE + DAYS_BIRTH + DAYS_EMPLOYED + OWN_CAR_AGE + OCCUPATION_TYPE + CNT_FAM_MEMBERS + ORGANIZATION_TYPE + AMR_REQ_CREDIT_BUREAU_SUM + AMT_CREDIT*ORGANIZATION_TYPE, family=binomial)
anova(reg_cd1, reg_int, test = "Chisq")
```


#Model Interpretation

```{r}
summary(pool(reg))
summary(pool(reg), conf.int=T)
```

#Model Limitation





